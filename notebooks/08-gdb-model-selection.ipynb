{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreter dir: /usr/bin/python3\n",
      "Working dir: /home/guillem/waterhack/water_hackathlon\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f'Interpreter dir: {sys.executable}')\n",
    "import os\n",
    "\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    os.chdir('../')\n",
    "    \n",
    "print(f'Working dir: {os.getcwd()}')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 100)\n",
    "from pandas_profiling import ProfileReport\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV,KFold,cross_val_score,ShuffleSplit,StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score,confusion_matrix,classification_report\n",
    "\n",
    "from scipy.stats import randint,uniform\n",
    "import lightgbm as lgb\n",
    "\n",
    "from waterhack.visualize import time_vs_y\n",
    "from waterhack.utils import find_col\n",
    "from waterhack.utils import series_to_supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%file model_evaluation.py\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bokeh.models import HoverTool\n",
    "from sklearn.neighbors.classification import KNeighborsClassifier\n",
    "from sklearn.manifold.t_sne import TSNE\n",
    "import umap\n",
    "try:\n",
    "    import holoviews as hv\n",
    "    import hvplot.streamz\n",
    "    import hvplot\n",
    "    import hvplot.pandas\n",
    "    from holoviews import streams\n",
    "    import streamz\n",
    "    from streamz.dataframe import DataFrame as StreamzDataFrame\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "def safe_margin(val, low=True, pct: float = 0.05):\n",
    "    low_pct, high_pct = 1 - pct, 1 + pct\n",
    "    func = min if low else max\n",
    "    return func(val * low_pct, val * high_pct)\n",
    "\n",
    "\n",
    "def safe_bounds(array, pct: float = 0.05):\n",
    "    low_x, high_x = array.min(), array.max()\n",
    "    low_x = safe_margin(low_x, pct=pct)\n",
    "    high_x = safe_margin(high_x, pct=pct, low=False)\n",
    "    return low_x, high_x\n",
    "def predict_grid(model, X):\n",
    "    x_grid, y_grid = example_meshgrid(X)\n",
    "    grid = np.c_[x_grid.ravel(), y_grid.ravel()]\n",
    "    probs = model.predict_proba(grid)[:, 1].reshape(x_grid.shape)\n",
    "    return probs, x_grid, y_grid\n",
    "def plot_confussion_matrix(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    target_names: list = None,\n",
    "    cmap: str = \"YlGnBu\",\n",
    "    width=500,\n",
    "    height: int = 400,\n",
    "    title: str = \"Confusion matrix\",\n",
    "    normalize: bool = False,\n",
    "):\n",
    "    value_label = \"examples\"\n",
    "    target_label = \"true_label\"\n",
    "    pred_label = \"predicted_label\"\n",
    "    label_color = \"color\"\n",
    "\n",
    "    def melt_distances_to_heatmap(distances: pd.DataFrame) -> pd.DataFrame:\n",
    "        dist_melt = pd.melt(distances.reset_index(), value_name=value_label, id_vars=\"index\")\n",
    "        dist_melt = dist_melt.rename(columns={\"index\": target_label, \"variable\": pred_label})\n",
    "        dist_melt[target_label] = pd.Categorical(dist_melt[target_label])\n",
    "        dist_melt[pred_label] = pd.Categorical(dist_melt[pred_label])\n",
    "        coords = dist_melt.copy()\n",
    "        coords[target_label] = dist_melt[target_label].values.codes\n",
    "        coords[pred_label] = dist_melt[pred_label].values.codes\n",
    "        return coords[[pred_label, target_label, value_label]]\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    if normalize:\n",
    "        conf_matrix = np.round(\n",
    "            conf_matrix.astype(\"float\") / conf_matrix.sum(axis=1)[:, np.newaxis], 3\n",
    "        )\n",
    "    # Adjust label color to make them readable when displayed on top of any colormap\n",
    "    df = melt_distances_to_heatmap(pd.DataFrame(conf_matrix))\n",
    "    mean = df[value_label].mean()\n",
    "    df[label_color] = -df[value_label].apply(lambda x: int(x > mean))\n",
    "    if target_names is not None:\n",
    "        df[target_label] = df[target_label].apply(lambda x: target_names[x])\n",
    "        df[pred_label] = df[pred_label].apply(lambda x: target_names[x])\n",
    "    true_label_name = \"Actual label\"\n",
    "    pred_label_name = \"Predicted label\"\n",
    "\n",
    "    tooltip = [\n",
    "        (true_label_name, \"@{%s}\" % target_label),\n",
    "        (pred_label_name, \"@{%s}\" % pred_label),\n",
    "        (\"Examples\", \"@{%s}\" % value_label),\n",
    "    ]\n",
    "    hover = HoverTool(tooltips=tooltip)\n",
    "    heatmap = hv.HeatMap(df, kdims=[pred_label, target_label])\n",
    "    heatmap.opts(title=title, colorbar=True, cmap=cmap, width=width, height=height, tools=[hover])\n",
    "    labeled = heatmap * hv.Labels(heatmap).opts(text_color=label_color, cmap=cmap)\n",
    "    return labeled.options(xlabel=pred_label_name, ylabel=true_label_name, invert_yaxis=True)\n",
    "\n",
    "\n",
    "def __plot_decision_boundaries(X, y, y_pred, resolution: int = 100, embedding=None):\n",
    "    if embedding is None:\n",
    "        embedding = TSNE(n_components=2, random_state=160290).fit_transform(X)\n",
    "\n",
    "    x_min, x_max = safe_bounds(embedding[:, 0])\n",
    "    y_min, y_max = safe_bounds(embedding[:, 1])\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(x_min, x_max, resolution), np.linspace(y_min, y_max, resolution)\n",
    "    )\n",
    "\n",
    "    # approximate Voronoi tesselation on resolution x resolution grid using 1-NN\n",
    "    background_model = KNeighborsClassifier(n_neighbors=1).fit(embedding, y_pred)\n",
    "    voronoi_bg = background_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    voronoi_bg = voronoi_bg.reshape((resolution, resolution))\n",
    "\n",
    "    mesh = hv.QuadMesh((xx, yy, voronoi_bg)).opts(cmap=\"viridis\")\n",
    "    points = hv.Scatter(\n",
    "        {\"x\": embedding[:, 0], \"y\": embedding[:, 1], \"pred\": y_pred, \"class\": y},\n",
    "        kdims=[\"x\", \"y\"],\n",
    "        vdims=[\"pred\", \"class\"],\n",
    "    )\n",
    "    errors = y_pred != y\n",
    "    failed_points = hv.Scatter(\n",
    "        {\"x\": embedding[errors, 0], \"y\": embedding[errors, 1]}, kdims=[\"x\", \"y\"]\n",
    "    ).opts(color=\"red\", size=5, alpha=0.9)\n",
    "\n",
    "    points = points.opts(\n",
    "        color=\"pred\", cmap=\"viridis\", line_color=\"grey\", size=10, alpha=0.8, tools=[\"hover\"]\n",
    "    )\n",
    "    plot = mesh * points * failed_points\n",
    "    plot = plot.opts(\n",
    "        xaxis=None, yaxis=None, width=500, height=450, title=\"Decision boundaries on TSNE\"\n",
    "    )\n",
    "    return plot\n",
    "\n",
    "\n",
    "def plot_decision_boundaries(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    y_pred_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    y_pred_test,\n",
    "    resolution: int = 100,\n",
    "    embedding=None,\n",
    "):\n",
    "    X = np.concatenate([X_train, X_test])\n",
    "    y = np.concatenate([y_train, y_test])\n",
    "    y_pred = np.concatenate([y_pred_train, y_pred_test])\n",
    "\n",
    "    if embedding is None:\n",
    "        try:\n",
    "            embedding = umap.UMAP(n_components=2, random_state=160290).fit_transform(X)\n",
    "        except:\n",
    "            from sklearn.manifold import TSNE\n",
    "\n",
    "            embedding = TSNE(n_components=2, random_state=160290).fit_transform(X)\n",
    "    x_min, x_max = safe_bounds(embedding[:, 0])\n",
    "    y_min, y_max = safe_bounds(embedding[:, 1])\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(x_min, x_max, resolution), np.linspace(y_min, y_max, resolution)\n",
    "    )\n",
    "\n",
    "    # approximate Voronoi tesselation on resolution x resolution grid using 1-NN\n",
    "    background_model = KNeighborsClassifier(n_neighbors=1).fit(embedding, y_pred)\n",
    "    voronoi_bg = background_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    voronoi_bg = voronoi_bg.reshape((resolution, resolution))\n",
    "\n",
    "    mesh = hv.QuadMesh((xx, yy, voronoi_bg)).opts(cmap=\"viridis\", alpha=0.6)\n",
    "    points_train = hv.Scatter(\n",
    "        {\n",
    "            \"x\": embedding[: len(y_train), 0],\n",
    "            \"y\": embedding[: len(y_train), 1],\n",
    "            \"pred\": y_pred_train,\n",
    "            \"class\": y_train,\n",
    "        },\n",
    "        kdims=[\"x\", \"y\"],\n",
    "        vdims=[\"pred\", \"class\"],\n",
    "    )\n",
    "    points_test = hv.Scatter(\n",
    "        {\n",
    "            \"x\": embedding[len(y_train) :, 0],\n",
    "            \"y\": embedding[len(y_train) :, 1],\n",
    "            \"pred\": y_pred_test,\n",
    "            \"class\": y_test,\n",
    "        },\n",
    "        kdims=[\"x\", \"y\"],\n",
    "        vdims=[\"pred\", \"class\"],\n",
    "    )\n",
    "    errors = y_pred != y\n",
    "    failed_points = hv.Scatter(\n",
    "        {\"x\": embedding[errors, 0], \"y\": embedding[errors, 1]}, kdims=[\"x\", \"y\"]\n",
    "    ).opts(color=\"red\", size=2, alpha=0.9)\n",
    "\n",
    "    points_train = points_train.opts(\n",
    "        color=\"class\", cmap=\"viridis\", line_color=\"grey\", size=10, alpha=0.8, tools=[\"hover\"]\n",
    "    )\n",
    "    points_test = points_test.opts(\n",
    "        color=\"class\",\n",
    "        cmap=\"viridis\",\n",
    "        line_color=\"grey\",\n",
    "        size=10,\n",
    "        alpha=0.8,\n",
    "        tools=[\"hover\"],\n",
    "        marker=\"square\",\n",
    "    )\n",
    "    plot = mesh * points_train * points_test * failed_points\n",
    "    plot = plot.opts(xaxis=None, yaxis=None, width=500, height=450, title=\"Fronteras de decisión\")\n",
    "    return plot\n",
    "\n",
    "\n",
    "def plot_classification_report(y, y_pred, **kwargs):\n",
    "    report = classification_report(y, y_pred, output_dict=True, **kwargs)\n",
    "    df = pd.DataFrame(report).applymap(lambda x: \"{:.2f}\".format(x))\n",
    "    df = df.T.reset_index()\n",
    "    return hv.Table(df).opts(title=\"Classification report\")\n",
    "\n",
    "\n",
    "def plot_feature_importances(model, target_names=None, feature_names=None, stacked: bool = False):\n",
    "    n_target, n_features = model.coef_.shape\n",
    "    ix = feature_names if feature_names is not None else list(range(n_features))\n",
    "    cols = (\n",
    "        target_names[:n_target]\n",
    "        if target_names is not None\n",
    "        else [\"class_{}\".format(i) for i in range(n_target)]\n",
    "    )\n",
    "    df = pd.DataFrame(index=ix, columns=cols, data=model.coef_.T)\n",
    "    df.index.name = \"Features\"\n",
    "    df.columns.name = \"output_class\"\n",
    "    bar = df.hvplot.bar(legend=True, stacked=stacked, rot=75)\n",
    "    bar = bar.opts(ylabel=\"Aggregated coefficients\", title=\"Feature importances\")\n",
    "    return bar\n",
    "\n",
    "\n",
    "def plot_model_evaluation(\n",
    "    model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    target_names=None,\n",
    "    feature_names=None,\n",
    "    normalize: bool = False,\n",
    "    resolution: int = 100,\n",
    "    stacked: bool = False,\n",
    "    embedding: np.ndarray = None,\n",
    "):\n",
    "    import panel as pn\n",
    "\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    metrics = plot_classification_report(y=y_test, y_pred=y_pred_test, target_names=target_names)\n",
    "    conf_mat = plot_confussion_matrix(\n",
    "        y_test=y_test, y_pred=y_pred_test, target_names=target_names, normalize=normalize\n",
    "    )\n",
    "    bounds = plot_decision_boundaries(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        y_pred_train=model.predict(X_train),\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        y_pred_test=model.predict(X_test),\n",
    "        resolution=resolution,\n",
    "        embedding=embedding,\n",
    "    )\n",
    "    # features = plot_feature_importances(\n",
    "    #     model=model, target_names=target_names, feature_names=feature_names, stacked=stacked\n",
    "    # )\n",
    "    gspec = pn.GridSpec(\n",
    "        min_height=700, height=700, max_height=1200, min_width=750, max_width=1980, width=750\n",
    "    )\n",
    "    gspec[0, 0] = bounds\n",
    "    gspec[1, 1] = metrics\n",
    "    gspec[1, 0] = pn.pane.HTML(str(model), margin=0)\n",
    "    gspec[0, 1] = conf_mat\n",
    "    # gspec[3:5, :] = features\n",
    "    return gspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 943668 entries, 0 to 943667\n",
      "Data columns (total 18 columns):\n",
      " #   Column        Non-Null Count   Dtype         \n",
      "---  ------        --------------   -----         \n",
      " 0   DMA           943668 non-null  object        \n",
      " 1   Timestamp     943668 non-null  datetime64[ns]\n",
      " 2   PressureBar   943668 non-null  float64       \n",
      " 3   m3Volume      943668 non-null  float64       \n",
      " 4   DateRaised    962 non-null     datetime64[ns]\n",
      " 5   LeakType      962 non-null     object        \n",
      " 6   DMAName       962 non-null     object        \n",
      " 7   is_leakage    943668 non-null  int64         \n",
      " 8   maxtempC      943668 non-null  int64         \n",
      " 9   mintempC      943668 non-null  int64         \n",
      " 10  totalSnow_cm  943668 non-null  float64       \n",
      " 11  sunHour       943668 non-null  float64       \n",
      " 12  uvIndex       943668 non-null  int64         \n",
      " 13  DewPointC     943668 non-null  int64         \n",
      " 14  humidity      943668 non-null  int64         \n",
      " 15  precipMM      943668 non-null  float64       \n",
      " 16  pressure      943668 non-null  int64         \n",
      " 17  tempC         943668 non-null  int64         \n",
      "dtypes: datetime64[ns](2), float64(5), int64(8), object(3)\n",
      "memory usage: 129.6+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DMA</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>PressureBar</th>\n",
       "      <th>m3Volume</th>\n",
       "      <th>DateRaised</th>\n",
       "      <th>LeakType</th>\n",
       "      <th>DMAName</th>\n",
       "      <th>is_leakage</th>\n",
       "      <th>maxtempC</th>\n",
       "      <th>mintempC</th>\n",
       "      <th>totalSnow_cm</th>\n",
       "      <th>sunHour</th>\n",
       "      <th>uvIndex</th>\n",
       "      <th>DewPointC</th>\n",
       "      <th>humidity</th>\n",
       "      <th>precipMM</th>\n",
       "      <th>pressure</th>\n",
       "      <th>tempC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEWSEVMA</td>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>2.011256</td>\n",
       "      <td>11.210</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1031</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NORW21MA</td>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>2.345946</td>\n",
       "      <td>8.094</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1031</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NEWSTUMA</td>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>2.564902</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1031</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BURYRDMA</td>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>2.208337</td>\n",
       "      <td>2.650</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1031</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NORW37MA</td>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>2.514855</td>\n",
       "      <td>2.791</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1031</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        DMA  Timestamp  PressureBar  m3Volume DateRaised LeakType DMAName  \\\n",
       "0  NEWSEVMA 2016-12-01     2.011256    11.210        NaT      NaN     NaN   \n",
       "1  NORW21MA 2016-12-01     2.345946     8.094        NaT      NaN     NaN   \n",
       "2  NEWSTUMA 2016-12-01     2.564902     0.000        NaT      NaN     NaN   \n",
       "3  BURYRDMA 2016-12-01     2.208337     2.650        NaT      NaN     NaN   \n",
       "4  NORW37MA 2016-12-01     2.514855     2.791        NaT      NaN     NaN   \n",
       "\n",
       "   is_leakage  maxtempC  mintempC  totalSnow_cm  sunHour  uvIndex  DewPointC  \\\n",
       "0           0         7         3           0.0      6.8        2          3   \n",
       "1           0         7         3           0.0      6.8        2          3   \n",
       "2           0         7         3           0.0      6.8        2          3   \n",
       "3           0         7         3           0.0      6.8        2          3   \n",
       "4           0         7         3           0.0      6.8        2          3   \n",
       "\n",
       "   humidity  precipMM  pressure  tempC  \n",
       "0        95       0.0      1031      4  \n",
       "1        95       0.0      1031      4  \n",
       "2        95       0.0      1031      4  \n",
       "3        95       0.0      1031      4  \n",
       "4        95       0.0      1031      4  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = pd.read_csv('data/processed/DMAVolumePressureWeather.csv', sep=\",\",parse_dates = ['Timestamp','DateRaised'])\n",
    "print(raw.info())\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DMA</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>PressureBar</th>\n",
       "      <th>m3Volume</th>\n",
       "      <th>DateRaised</th>\n",
       "      <th>LeakType</th>\n",
       "      <th>DMAName</th>\n",
       "      <th>is_leakage</th>\n",
       "      <th>maxtempC</th>\n",
       "      <th>mintempC</th>\n",
       "      <th>totalSnow_cm</th>\n",
       "      <th>sunHour</th>\n",
       "      <th>uvIndex</th>\n",
       "      <th>DewPointC</th>\n",
       "      <th>humidity</th>\n",
       "      <th>precipMM</th>\n",
       "      <th>pressure</th>\n",
       "      <th>tempC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEWSEVMA</td>\n",
       "      <td>2016-12-01 00:00:00</td>\n",
       "      <td>2.011256</td>\n",
       "      <td>11.210000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1031</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NEWSEVMA</td>\n",
       "      <td>2016-12-01 00:15:00</td>\n",
       "      <td>1.986232</td>\n",
       "      <td>10.410000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1031</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NEWSEVMA</td>\n",
       "      <td>2016-12-01 00:30:00</td>\n",
       "      <td>2.051919</td>\n",
       "      <td>10.100000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1031</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NEWSEVMA</td>\n",
       "      <td>2016-12-01 00:45:00</td>\n",
       "      <td>2.073815</td>\n",
       "      <td>9.970000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1030</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NEWSEVMA</td>\n",
       "      <td>2016-12-01 01:00:00</td>\n",
       "      <td>1.979976</td>\n",
       "      <td>9.370000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1030</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943613</th>\n",
       "      <td>NEWSEVMA</td>\n",
       "      <td>2020-05-31 23:00:00</td>\n",
       "      <td>2.110763</td>\n",
       "      <td>9.030000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1026</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943634</th>\n",
       "      <td>NEWSEVMA</td>\n",
       "      <td>2020-05-31 23:15:00</td>\n",
       "      <td>2.113888</td>\n",
       "      <td>8.569999</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1026</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943641</th>\n",
       "      <td>NEWSEVMA</td>\n",
       "      <td>2020-05-31 23:30:00</td>\n",
       "      <td>2.057624</td>\n",
       "      <td>8.640000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1026</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943648</th>\n",
       "      <td>NEWSEVMA</td>\n",
       "      <td>2020-05-31 23:45:00</td>\n",
       "      <td>2.048247</td>\n",
       "      <td>8.180000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1026</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943657</th>\n",
       "      <td>NEWSEVMA</td>\n",
       "      <td>2020-06-01 00:00:00</td>\n",
       "      <td>2.129517</td>\n",
       "      <td>7.290000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1026</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122664 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             DMA           Timestamp  PressureBar   m3Volume DateRaised  \\\n",
       "0       NEWSEVMA 2016-12-01 00:00:00     2.011256  11.210000        NaT   \n",
       "10      NEWSEVMA 2016-12-01 00:15:00     1.986232  10.410000        NaT   \n",
       "16      NEWSEVMA 2016-12-01 00:30:00     2.051919  10.100000        NaT   \n",
       "21      NEWSEVMA 2016-12-01 00:45:00     2.073815   9.970000        NaT   \n",
       "27      NEWSEVMA 2016-12-01 01:00:00     1.979976   9.370000        NaT   \n",
       "...          ...                 ...          ...        ...        ...   \n",
       "943613  NEWSEVMA 2020-05-31 23:00:00     2.110763   9.030000        NaT   \n",
       "943634  NEWSEVMA 2020-05-31 23:15:00     2.113888   8.569999        NaT   \n",
       "943641  NEWSEVMA 2020-05-31 23:30:00     2.057624   8.640000        NaT   \n",
       "943648  NEWSEVMA 2020-05-31 23:45:00     2.048247   8.180000        NaT   \n",
       "943657  NEWSEVMA 2020-06-01 00:00:00     2.129517   7.290000        NaT   \n",
       "\n",
       "       LeakType DMAName  is_leakage  maxtempC  mintempC  totalSnow_cm  \\\n",
       "0           NaN     NaN           0         7         3           0.0   \n",
       "10          NaN     NaN           0         7         3           0.0   \n",
       "16          NaN     NaN           0         7         3           0.0   \n",
       "21          NaN     NaN           0         7         3           0.0   \n",
       "27          NaN     NaN           0         7         3           0.0   \n",
       "...         ...     ...         ...       ...       ...           ...   \n",
       "943613      NaN     NaN           0        19        10           0.0   \n",
       "943634      NaN     NaN           0        19        10           0.0   \n",
       "943641      NaN     NaN           0        19        10           0.0   \n",
       "943648      NaN     NaN           0        18        10           0.0   \n",
       "943657      NaN     NaN           0        18        10           0.0   \n",
       "\n",
       "        sunHour  uvIndex  DewPointC  humidity  precipMM  pressure  tempC  \n",
       "0           6.8        2          3        95       0.0      1031      4  \n",
       "10          6.8        2          3        95       0.0      1031      4  \n",
       "16          6.8        2          3        95       0.0      1031      4  \n",
       "21          6.8        2          3        95       0.0      1030      3  \n",
       "27          6.8        2          3        95       0.0      1030      3  \n",
       "...         ...      ...        ...       ...       ...       ...    ...  \n",
       "943613     16.7        5          9        94       0.0      1026     10  \n",
       "943634     16.7        5          9        94       0.0      1026     10  \n",
       "943641     16.7        5          9        94       0.0      1026     10  \n",
       "943648     16.7        5         10        95       0.0      1026     10  \n",
       "943657     16.7        5         10        95       0.0      1026     10  \n",
       "\n",
       "[122664 rows x 18 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_DMA = \"NEWSEVMA\"\n",
    "df_filt = raw[raw.DMA == selected_DMA]\n",
    "df_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filt[[\"is_leakage\", \"PressureBar\", \"m3Volume\", \"Timestamp\"]].to_csv(\"data_dma.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_features(x):\n",
    "    x[\"day\"] = x[\"Timestamp\"].map(lambda x:x.day)\n",
    "    x[\"month\"] = x[\"Timestamp\"].map(lambda x:x.month)\n",
    "    x[\"weekday\"] = x[\"Timestamp\"].map(lambda x:x.weekday())\n",
    "    return x.drop(\"Timestamp\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = add_time_features(df_filt[[\"Timestamp\"]].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"is_leakage\", \"PressureBar\", \"m3Volume\"]\n",
    "df = df_filt[columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Mark outliers to be filtered\n",
    "outlier_scaler = StandardScaler().fit(df.values[:, 1:])\n",
    "no_outliers = (np.abs(outlier_scaler.transform(df.values[:, 1:])) < 2).all(1)\n",
    "# Scaler for data with no outliers\n",
    "feature_scaler = StandardScaler().fit(df.values[no_outliers, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = df_filt[[\"is_leakage\", \"PressureBar\", \"m3Volume\", \"Timestamp\"]].set_index(\"Timestamp\")[no_outliers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_outlier(x):\n",
    "    val = outlier_scaler.transform(x)\n",
    "    return (np.abs(val).flatten() > 2).any()\n",
    "\n",
    "def create_examples(input_data, times, tw, ignore_leaks=False):\n",
    "    inout_seq = []\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L-tw):\n",
    "        train_seq = input_data[i:i+tw, 1:]\n",
    "        train_label = int(input_data[i+tw - 6 * 4:i+tw, 0].flatten().any())\n",
    "        #train_label = int(input_data[i+int(tw / 4):i+tw, 0].flatten().any())\n",
    "        #mprof = create_matrix_profile(train_seq, 24)\n",
    "        times_seq = times[i:i+tw]\n",
    "        if is_outlier(train_seq):\n",
    "            continue\n",
    "        \n",
    "        inputs.append(np.concatenate([#mprof,\n",
    "                                     feature_scaler.transform(train_seq).flatten(),\n",
    "                                     times_seq.flatten()]))\n",
    "        outputs.append(train_label)\n",
    "    return np.array(inputs), np.array(outputs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# I didnt use this but it can be useful to extract featuress from time series\n",
    "import stumpy\n",
    "def create_matrix_profile(x, window):\n",
    "    return np.concatenate([stumpy.stump(x[:, 0], m=window).flatten(),\n",
    "                           stumpy.stump(x[:, 1], m=window).flatten()])\n",
    "                           \n",
    "def create_categorical_index(x, offset=0): # index of matrix profile categorical values\n",
    "    v = np.zeros(x.shape, dtype=bool)\n",
    "    v[:, 1:] = True\n",
    "    return np.arange(offset, x.shape[0]*x.shape[1]+ offset)[v.flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from imblearn.datasets import make_imbalance\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV,KFold,cross_val_score,ShuffleSplit,StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score,confusion_matrix,classification_report\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, y = create_examples(df.values, times.values, 48*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data/processed/features\", x.astype(np.float32))\n",
    "np.save(\"data/processed/targets\", y.astype(np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"model_series_12h.pck\", \"rb\") as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_2 = lgb.LGBMClassifier(objective='binary',metric='binary',bagging_fraction=0.5,\n",
    "                         categorical_features =cats,\n",
    "                         n_estimators=150,\n",
    "                         is_unbalance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = lgb.LGBMClassifier(objective='binary',metric='binary',bagging_fraction=0.5,\n",
    "                         categorical_features =cats,\n",
    "                         n_estimators=100,\n",
    "                         is_unbalance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x, y, random_state=42)\n",
    "\n",
    "print('Training target statistics: {}'.format(Counter(y_train)))\n",
    "print('Testing target statistics: {}'.format(Counter(y_test)))\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = make_pipeline(NearMiss(version=2), LogisticRegression(max_iter=200, class_weight='balanced'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm.fit(X_train, y_train)\n",
    "\n",
    "# Classify and report the results\n",
    "print(classification_report_imbalanced(y_test, gbm.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_2.fit(X_train, y_train)\n",
    "\n",
    "# Classify and report the results\n",
    "print(classification_report_imbalanced(y_test, gbm_2.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = gbm_2.predict(X_train)\n",
    "y_test_pred = gbm_2.predict(X_test)\n",
    "\n",
    "print(\"TRAIN dataset\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print(confusion_matrix(y_train, y_train_pred))\n",
    "\n",
    "print(\"TEST DATSET\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.46\n",
    "y_train_pred = (model.predict_proba(X_train)[:, 1] > p).astype(int)\n",
    "y_test_pred = (model.predict_proba(X_test)[:, 1] > p).astype(int)\n",
    "\n",
    "print(\"TRAIN dataset\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print(confusion_matrix(y_train, y_train_pred))\n",
    "\n",
    "print(\"TEST DATSET\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1, t2 = 0.2, 0.55\n",
    "y_train_pred = np.logical_and(gbm_2.predict_proba(X_train)[:, 1] > t1,\n",
    "                              gbm.predict_proba(X_train)[:, 1] > t2).astype(int)\n",
    "y_test_pred = np.logical_and(gbm_2.predict_proba(X_test)[:, 1] > t1,\n",
    "                             gbm.predict_proba(X_test)[:, 1] > t2).astype(int)\n",
    "\n",
    "print(\"TRAIN dataset\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print(confusion_matrix(y_train, y_train_pred))\n",
    "\n",
    "print(\"TEST DATSET\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "params = {\n",
    "        'n_estimators': randint(100, 1200),\n",
    "        }\n",
    "# Split training set into validation\n",
    "#cv_ss = TimeSeriesSplit(n_splits=10).split(X_train, y_train)\n",
    "cv_ss = StratifiedKFold(n_splits=5).split(X_train, y_train)\n",
    "# Define model\n",
    "n_iter = 20 # Max numero de iteraciones\n",
    "random_search = RandomizedSearchCV(gbm, param_distributions=params,\n",
    "                                   n_iter = n_iter, n_jobs=4,\n",
    "                                   scoring = \"f1_macro\",\n",
    "                                   cv = cv_ss, verbose=10, random_state=1234 )\n",
    "# Fit xgb\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n Best hyperparameters:')\n",
    "print(random_search.best_params_)\n",
    "gbm_model = random_search.best_estimator_\n",
    "print('Best model:')\n",
    "print(gbm_model)\n",
    "\n",
    "# Collect GridSearch CV results. Get the best estimator.\n",
    "df_results = pd.DataFrame(random_search.cv_results_)\n",
    "err = df_results[df_results.rank_test_score == 1].filter(regex=(\"split\\d.*\")).values\n",
    "display(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"model_matrices_12h.pck\", \"wb\") as f:\n",
    "    pickle.dump(gbm_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model_series_12h.pck\", \"rb\") as f:\n",
    "    pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = gbm_model.predict(X_train)\n",
    "y_test_pred = gbm_model.predict(X_test)\n",
    "\n",
    "print(\"TRAIN dataset\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print(confusion_matrix(y_train, y_train_pred))\n",
    "\n",
    "print(\"TEST DATSET\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n Best hyperparameters:')\n",
    "print(random_search.best_params_)\n",
    "gbm_model = random_search.best_estimator_\n",
    "print('Best model:')\n",
    "print(gbm_model)\n",
    "\n",
    "# Collect GridSearch CV results. Get the best estimator.\n",
    "df_results = pd.DataFrame(random_search.cv_results_)\n",
    "err = df_results[df_results.rank_test_score == 1].filter(regex=(\"split\\d.*\")).values\n",
    "display(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = gbm_model.predict(X_train)\n",
    "y_test_pred = gbm_model.predict(X_test)\n",
    "\n",
    "print(\"TRAIN dataset\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print(confusion_matrix(y_train, y_train_pred))\n",
    "\n",
    "print(\"TEST DATSET\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n Best hyperparameters:')\n",
    "print(random_search.best_params_)\n",
    "gbm_model = random_search.best_estimator_\n",
    "print('Best model:')\n",
    "print(gbm_model)\n",
    "\n",
    "# Collect GridSearch CV results. Get the best estimator.\n",
    "df_results = pd.DataFrame(random_search.cv_results_)\n",
    "err = df_results[df_results.rank_test_score == 1].filter(regex=(\"split\\d.*\")).values\n",
    "display(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = gbm_model.predict(X_train)\n",
    "y_test_pred = gbm_model.predict(X_test)\n",
    "\n",
    "print(\"TRAIN dataset\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print(confusion_matrix(y_train, y_train_pred))\n",
    "\n",
    "print(\"TEST DATSET\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "def val_recall(t1, t2):\n",
    "    y_train_pred = np.logical_and(gbm_2.predict_proba(X_train)[:, 1] > t1,\n",
    "                              gbm.predict_proba(X_train)[:, 1] > t2).astype(int)\n",
    "    y_test_pred = np.logical_and(gbm_2.predict_proba(X_test)[:, 1] > t1,\n",
    "                                 gbm.predict_proba(X_test)[:, 1] > t2).astype(int)\n",
    "    return (precision_score(y_train, y_train_pred),\n",
    "            precision_score(y_test, y_test_pred),\n",
    "            recall_score(y_train, y_train_pred),\n",
    "            recall_score(y_test, y_test_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx,yy = np.meshgrid(np.arange(0.05, 0.95, 0.05), np.arange(0.5, 0.95, 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_recs = []\n",
    "eval_recs = []\n",
    "train_precs = []\n",
    "eval_precs = []\n",
    "for t1, t2 in tqdm(list(zip(xx.ravel(), yy.ravel()))):\n",
    "    tra, tes, ptra, ptes = val_recall(t1, t2)\n",
    "    train_recs.append(tra)\n",
    "    eval_recs.append(tes)\n",
    "    train_precs.append(ptra)\n",
    "    eval_precs.append(ptes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.plot_surface(xx, yy, np.array(eval_precs).reshape(xx.shape), rstride=1, cstride=1, cmap=cm.inferno)\n",
    "ax.plot_surface(xx, yy, np.array(eval_recs).reshape(xx.shape), rstride=1, cstride=1, cmap=cm.viridis, alpha=0.9)\n",
    "plt.xlabel(\"x\")\n",
    "ax.set_zlim(.1, 0.8)\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.plot_surface(xx, yy, np.array(eval_precs).reshape(xx.shape), rstride=1, cstride=1, cmap=cm.viridis)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stumpy\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "your_time_series = np.random.normal(size=(4 * 24))\n",
    "window_size = 12  # Approximately, how many data points might be found in a pattern\n",
    "#all_gpu_devices = [device.id for device in cuda.list_devices()]  # Get a list of all available GPU devices\n",
    "\n",
    "matrix_profile = stumpy.stump(your_time_series, m=window_size)#, device_id=all_gpu_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, input_size=5, hidden_layer_size=256, output_size=1, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size, num_layers=num_layers)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "        self.hidden_cell = (torch.zeros(num_layers,1,self.hidden_layer_size),\n",
    "                            torch.zeros(num_layers,1,self.hidden_layer_size))\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        time_steps, bs, n_feats = input_seq.shape\n",
    "        x = input_seq.view(len(input_seq) ,bs, -1)\n",
    "        #print(x.shape, input_seq.shape)\n",
    "        lstm_out, self.hidden_cell = self.lstm(x, self.hidden_cell)\n",
    "        #print(lstm_out.shape)\n",
    "        lstm_out = lstm_out.transpose(1, 0).reshape(bs * time_steps, -1)\n",
    "        #print(\"lstm\", lstm_out.shape)\n",
    "        predictions = self.linear(lstm_out).reshape(bs, time_steps, self.output_size)[:, -1, :]\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = MyLSTM(output_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lstm(torch.FloatTensor(seqs[0][0]).unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(seqs[0][0]).unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM().to(DEVICE)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 1500\n",
    "bs = 32\n",
    "\n",
    "for i in trange(epochs):\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "    for j, (seq, labels) in enumerate(seqs):\n",
    "        input_batch.append(seq.unsqueeze(1))\n",
    "        target_batch.append(labels.unsqueeze(1))\n",
    "        if len(input_batch) == bs:\n",
    "            optimizer.zero_grad()\n",
    "            model.hidden_cell = (torch.zeros(model.num_layers, bs, model.hidden_layer_size).to(DEVICE),\n",
    "                            torch.zeros(model.num_layers, bs, model.hidden_layer_size).to(DEVICE))\n",
    "            #print(seq.shape, labels.shape)\n",
    "            inps = torch.cat(input_batch, 1)\n",
    "            tgts = torch.cat(target_batch, 1)\n",
    "            #print(inps.shape, tgts.shape, len(input_batch), j)\n",
    "            y_pred = model(inps)\n",
    "            #print(y_pred.shape, tgts[0, :, 1].shape)\n",
    "\n",
    "            single_loss = loss_function(y_pred, tgts[0, :, 0].unsqueeze(1))\n",
    "            single_loss.backward()\n",
    "            optimizer.step()\n",
    "            input_batch = []\n",
    "            target_batch = []\n",
    "\n",
    "    if True:#i%25 == 1:\n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n",
    "\n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "640 / 32 / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "targets = []\n",
    "for seq, labels in tqdm(seqs[:10000]):\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden_cell = (torch.zeros(2, 1, model.hidden_layer_size).to(DEVICE),\n",
    "                        torch.zeros(2, 1, model.hidden_layer_size).to(DEVICE))\n",
    "        #print(seq.shape, labels.shape)\n",
    "        y_pred = model(seq.unsqueeze(1))\n",
    "        #print(y_pred.shape, labels.shape)\n",
    "        preds.append(y_pred)\n",
    "        targets.append(labels)\n",
    "        #single_loss = loss_function(y_pred, labels)\n",
    "        #single_loss.backward()\n",
    "        #optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
